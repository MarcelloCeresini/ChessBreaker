{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "try_save_load_model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO9+ccafJ0/XW/LqM/r/bXN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarcelloCeresini/ChessBreaker/blob/main/try_save_load_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PRIVATE CELL\n",
        "username = 'MarcelloCeresini'\n",
        "repository = 'ChessBreaker'"
      ],
      "metadata": {
        "id": "H6d5hfwVzjvK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NX2IwqXzhlV",
        "outputId": "0addc061-1822-4103-8fce-92999f43f626"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ray in /usr/local/lib/python3.7/dist-packages (1.13.0)\n",
            "Requirement already satisfied: anytree in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Requirement already satisfied: chess in /usr/local/lib/python3.7/dist-packages (1.9.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray) (1.0.4)\n",
            "Requirement already satisfied: virtualenv in /usr/local/lib/python3.7/dist-packages (from ray) (20.16.3)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray) (22.1.0)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.7/dist-packages (from ray) (1.2.0)\n",
            "Requirement already satisfied: click<=8.0.4,>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray) (7.1.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray) (4.3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray) (3.7.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.7/dist-packages (from ray) (1.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray) (3.13)\n",
            "Requirement already satisfied: grpcio<=1.43.0,>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray) (1.43.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray) (1.21.6)\n",
            "Requirement already satisfied: protobuf<4.0.0,>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray) (3.17.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray) (2.23.0)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio<=1.43.0,>=1.28.1->ray) (1.15.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (4.12.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (5.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (4.1.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (0.18.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->ray) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (2022.6.15)\n",
            "Requirement already satisfied: platformdirs<3,>=2.4 in /usr/local/lib/python3.7/dist-packages (from virtualenv->ray) (2.5.2)\n",
            "Requirement already satisfied: distlib<1,>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from virtualenv->ray) (0.3.5)\n",
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n",
            "fatal: destination path 'ChessBreaker' already exists and is not an empty directory.\n",
            "/content/ChessBreaker\n",
            "elo_eval_endgame.py  \u001b[0m\u001b[01;34mold_stuff\u001b[0m/                \u001b[01;34mside_stuff\u001b[0m/\n",
            "elo_eval.ipynb       \u001b[01;34m__pycache__\u001b[0m/              try_a_match.ipynb\n",
            "main.py              random_endgame_gen.ipynb  try_save_load_model.ipynb\n",
            "\u001b[01;34mmodel_checkpoints\u001b[0m/   README.md                 utils.py\n",
            "model.py             requirements.txt\n",
            "notes.txt            \u001b[01;34mresults\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    !pip3 install ray anytree chess\n",
        "    !nvidia-smi             # Check which GPU has been chosen for us\n",
        "    !rm -rf logs\n",
        "    #from google.colab import drive\n",
        "    #drive.mount('/content/drive')\n",
        "    #%cd /content/drive/MyDrive/GitHub/\n",
        "    !git clone https://github.com/{username}/{repository}.git\n",
        "    %cd {repository}\n",
        "    %ls\n",
        "except:\n",
        "    IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pickle import TRUE\n",
        "import absl.logging\n",
        "from matplotlib.pyplot import step\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "import numpy as np\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "absl.logging.set_verbosity(absl.logging.ERROR)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "tf.get_logger().setLevel(0)\n",
        "# tf.autograph.set_verbosity(1)\n",
        "warnings.filterwarnings('ignore')\n",
        "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)  \n",
        "\n",
        "import chess\n",
        "from anytree import Node\n",
        "import datetime\n",
        "from time import time\n",
        "from tqdm import tqdm\n",
        "from queue import Queue\n",
        "import ray\n",
        "import pickle\n",
        "\n",
        "\n",
        "ray.shutdown()\n",
        "ray.init(\n",
        "    log_to_driver = False,  # comment to see logs from workers\n",
        "    include_dashboard=False)         # to remove objects in ray's object_store_memory that are no longer in use\n",
        "\n",
        "from numpy.random import default_rng\n",
        "rng = default_rng()\n",
        "\n",
        "import utils\n",
        "from utils import plane_dict, Config, x_y_from_position\n",
        "from model import create_model, LogitsMaskToSoftmax\n",
        "\n",
        "conf = Config()\n",
        "# print(ray.available_resources())\n",
        "\n",
        "dataset_train = tf.data.TextLineDataset(conf.PATH_ENDGAME_TRAIN_DATASET).shuffle(10000).prefetch(tf.data.AUTOTUNE).repeat()\n",
        "# using np.arrays because we add chunks of data and not one at a time (O(n) to move all data is actually O(n/m), with m chunk size)\n",
        "# and also we need to sample randomly batches of data, that for linked lists (like queue) is O(n*batch_zize), instead for arrays is O(batch_size)\n",
        "\n",
        "exp_buffer = utils.ExperienceBuffer(conf.MAX_BUFFER_SIZE)\n",
        "\n",
        "\n",
        "class MyNode(Node): # subclassing Node from Anytree to add some methods\n",
        "\n",
        "    def update_action_value(self, new_action_value):                                                        # used during backtracking to update action value if the simulation reached the end through that node\n",
        "        self.action_value += (new_action_value-self.action_value)/(self.visit_count+1)                      # simply the mean value, but computed iteratively\n",
        "\n",
        "    def calculate_upper_confidence_bound(self, num_total_iterations=1):                                     # Q + U --> U proportional to P/(1+N) --> parameter decides exploration vs. exploitation\n",
        "        new_UCF = self.action_value + conf.expl_param(num_total_iterations)*self.prior/(1+self.visit_count)\n",
        "        return new_UCF\n",
        "\n",
        "    def calculate_move_probability(self, num_move=1):                                           # N^(1/tau) --> tau is a temperature parameter (exploration vs. exploitation)\n",
        "        return self.visit_count**(1/conf.temp_param(num_move))\n",
        "\n",
        "\n",
        "def MTCS(model, root_node, max_depth, num_restarts):\n",
        "    '''\n",
        "        The search descends until it finds a leaf to be evalued, then restarts until it gathers a batch of evaluations, then expands all the nodes in the batch\n",
        "        If the maximum depth is reached, the algorithm then backpropagates the action value of the reached node back to the root of the tree\n",
        "        Exploration incentivised by the decrease of action value / upper confidence bound with each visit to the node\n",
        "        Exploitation incentivised by the two parameters temp_param and expl_param (respectively to choose almost surely the most probable move, and to focus more on the action value rather than the prior of the node)\n",
        "    '''\n",
        "    INIT_ROOT = root_node\n",
        "    nodes_to_visit = Queue()\n",
        "    # number of times to explore up until max_depth\n",
        "    i = 0\n",
        "\n",
        "    leaf_node_batch = []\n",
        "    legal_moves_batch = []\n",
        "    \n",
        "    while i < num_restarts:\n",
        "        if nodes_to_visit.empty():\n",
        "            root_node = INIT_ROOT\n",
        "            i+=1\n",
        "        else:\n",
        "            root_node = nodes_to_visit.get_nowait()\n",
        "        # print(i, root_node.name, root_node.depth, root_node.visit_count)\n",
        "        \n",
        "        step_down = True\n",
        "        control_counter = 0\n",
        "        while step_down:\n",
        "            control_counter+=1\n",
        "            if control_counter > 2*max_depth: \n",
        "                print(\"stuck in loop, leaving\")\n",
        "                break # bigger margin, but if it is stuk in a loop for some reason, at least it leaves\n",
        "            \n",
        "            # assert root_node.depth >= 0 and root_node.depth <= max_depth, \"depth is wrong\"\n",
        "            if root_node.is_leaf and not root_node.is_finish_position:                                                                           # if it's leaf --> need to pass the position (planes) through the model, to get priors (action_values) and outcome (state_value)\n",
        "                step_down = False\n",
        "\n",
        "                if len(root_node.siblings) > 0:         # this part is to try and avoid batching the same node twice (so we evaluate a random sibling instead)\n",
        "                    leaf_node_list = [node.board_history for node in leaf_node_batch]\n",
        "                    if root_node.board_history in leaf_node_list:\n",
        "                        siblings_list = list(root_node.siblings)\n",
        "                        random_sibling = np.random.choice(siblings_list)\n",
        "                        while random_sibling.board_history in leaf_node_list and len(siblings_list)>1:\n",
        "                            siblings_list.remove(random_sibling) #do it with np.random in a range, and POP instead of remove\n",
        "                            random_sibling = np.random.choice(siblings_list)\n",
        "                        \n",
        "                        root_node = random_sibling\n",
        "                        \n",
        "                # important! save legal moves AFTER choosing root_node (they have to be the legal moves available in that position)\n",
        "                legal_moves = list(root_node.board.legal_moves)\n",
        "\n",
        "                if len(legal_moves) == 0: # if you can't move, the match is finished\n",
        "                    step_down = False\n",
        "                    final_outcome = root_node.board.outcome(claim_draw=True)\n",
        "\n",
        "                    if final_outcome != None:\n",
        "                        root_node.is_finish_position = True\n",
        "                        if final_outcome.winner == None:\n",
        "                            # ACTION_VALUE ASSIGNATION\n",
        "                            root_node.action_value = 0\n",
        "                        else:\n",
        "                            # ACTION_VALUE ASSIGNATION\n",
        "                            root_node.action_value = int(final_outcome.winner)*2-1 # winner white = 1, black =0 --> we want +1 and -1\n",
        "                    else:\n",
        "                        print(\"Something's wrong\")\n",
        "                else: # if instead there are legal moves, keep going and expand it\n",
        "                    leaf_node_batch.append(root_node)\n",
        "                    legal_moves_batch.append(legal_moves)\n",
        "                \n",
        "                root_node.visit_count += 1\n",
        "\n",
        "                # print(\"to be evaluated\", \"d\", root_node.depth, \"vc\", root_node.visit_count, \"name\", root_node.name)\n",
        "\n",
        "                if len(leaf_node_batch) == conf.BATCH_DIM or root_node == INIT_ROOT:\n",
        "                    # in order to avoid creating multiple times the children of the same node, we only keep unique values\n",
        "                    if len(set(leaf_node_batch)) < conf.BATCH_DIM:\n",
        "                        leaf_node_batch, legal_moves_batch = utils.reduce_repetitions(leaf_node_batch, legal_moves_batch)                    \n",
        "                    \n",
        "                    plane_list = [root_node.planes for root_node in leaf_node_batch]\n",
        "                    # 0.0032072067260742188\n",
        "                    # 7.05718994140625e-05\n",
        "                    legal_moves_list = [utils.scatter_idxs(utils.mask_moves_flatten(legal_moves)) for legal_moves in legal_moves_batch]\n",
        "\n",
        "                    planes = np.stack(plane_list)\n",
        "                    legal_moves_input = np.stack(legal_moves_list)\n",
        "\n",
        "                    full_moves_batch, outcome_batch = model([planes, legal_moves_input])\n",
        "\n",
        "                    full_moves_batch_np = full_moves_batch.numpy()\n",
        "                    # print(np.shape(full_moves_batch_np[0]))\n",
        "                    outcome_batch_np = outcome_batch.numpy()\n",
        "\n",
        "                    if np.shape(full_moves_batch_np)[0] != 1:\n",
        "                        full_moves_batch_np = np.moveaxis(full_moves_batch.numpy(), 0, 0)\n",
        "                        outcome_batch_np = np.moveaxis(outcome_batch.numpy(), 0, 0)\n",
        "                    \n",
        "                    for root_node, full_moves, outcome, legal_moves in zip(leaf_node_batch, full_moves_batch_np, outcome_batch_np, legal_moves_batch):\n",
        "                        nodes_to_visit.put_nowait(root_node)\n",
        "                        \n",
        "                        mask_idx = utils.mask_moves_flatten(legal_moves)\n",
        "                        \n",
        "                        # we want all the priors POSITIVE in order to comply with training\n",
        "                        # so we swap the signs inside the model when it's black's turn\n",
        "                        # since we want the value of the nodes (and so, also the priors) in which black wins\n",
        "                        # to be negative, we need to swap them again\n",
        "                        turn = root_node.board.turn*2-1 # white = 1, black = -1\n",
        "                        priors = [full_moves[idx]*turn for idx in mask_idx]                        # boolean mask returns a tensor of only the values that were masked (as a list let's say)\n",
        "                                                \n",
        "                        # 0.006434917449951172\n",
        "                        # 1.3113021850585938e-05\n",
        "                        root_node.action_value = outcome    \n",
        "                        \n",
        "                        # TODO: maybe a second training without this? \n",
        "                        if root_node == INIT_ROOT:  # increase exploration at the root, since if the network is not good it will not make good starting choices\n",
        "                            dir_noise = rng.dirichlet([conf.ALPHA_DIRICHLET]*len(priors))\n",
        "                            priors = [((1-conf.EPS_NOISE)*p + conf.EPS_NOISE*noise) for p, noise in zip(priors, dir_noise)]\n",
        "\n",
        "                        for move, prior in zip(legal_moves, priors):                                                # creating children\n",
        "\n",
        "                            root_board_fen = root_node.board.fen()\n",
        "                            new_board = chess.Board()\n",
        "                            new_board.set_fen(root_board_fen)\n",
        "                            new_board.push(move)\n",
        "                                                        \n",
        "                            new_board_history = root_node.board_history.copy()                                      # and board history! (copy because list are pointers)\n",
        "                            new_board_history.append(new_board.fen()[:-6])\n",
        "\n",
        "                            planes = utils.update_planes(root_node.planes, new_board, new_board_history)\n",
        "                            \n",
        "                            MyNode(\n",
        "                                move.uci(), \n",
        "                                parent = root_node,                                                                 # very important to build the tree\n",
        "                                prior = prior,                                                                      # prior is the \"initial\" state_value of a node\n",
        "                                visit_count = 0,                                                                    # initialize visit_count to 0\n",
        "                                action_value = 0,\n",
        "                                is_finish_position = False,\n",
        "                                board = new_board, \n",
        "                                board_history = new_board_history,                                                  \n",
        "                                planes = planes             # update the planes --> each node stores its input planes!\n",
        "                            )\n",
        "\n",
        "                    leaf_node_batch = []\n",
        "                    legal_moves_batch = []\n",
        "\n",
        "            else: # if it does not need to be evalued because it already has children \n",
        "                if root_node.depth < max_depth and not root_node.is_finish_position:                                # if we are normally descending\n",
        "                    # print(\"choosing point\", \"d\", root_node.depth, \"vc\", root_node.visit_count, \"name\", root_node.name)\n",
        "                    children = root_node.children                                                               # get all the children (always != [])\n",
        "                    \n",
        "                    values = [child.calculate_upper_confidence_bound(i) for child in children]  # we pass the restart number (i) to the function --> decrease exploration\n",
        "                    if INIT_ROOT.board.turn == chess.WHITE:\n",
        "                        root_node = children[np.argmax(values)]\n",
        "                    else:\n",
        "                        root_node = children[np.argmin(values)]\n",
        "                    root_node.visit_count += 1                                                                  # add 1 to the visit count of the chosen child\n",
        "                    # print(\"chosen node\", \"d\", root_node.depth, \"vc\", root_node.visit_count, \"name\", root_node.name)\n",
        "                else:\n",
        "                    step_down = False                                # it will leave the while, max depth is reached\n",
        "                    # print(\"final leaf\", \"d\", root_node.depth, \"vc\", root_node.visit_count, \"name\", root_node.name, root_node.calculate_upper_confidence_bound())\n",
        "                    outcome = root_node.action_value    # needed for when depth=max_depth AND NOT LEAF (that means, already visited leaf) --> don't REDO the evaluation, it would give the same result, simply copy it from before\n",
        "                    # barckpropagation of action value through the tree\n",
        "                    while root_node.depth > 0:\n",
        "                        # root node should be an already evalued leaf, at max depth (so OUTCOME has been set)\n",
        "                        # assert root_node.depth > 0 and root_node.depth <= max_depth, \"depth is wrong\"\n",
        "                        root_node = root_node.parent\n",
        "                        root_node.update_action_value(outcome)\n",
        "\n",
        "    return INIT_ROOT\n",
        "\n",
        "\n",
        "def choose_move(root_node, num_move):\n",
        "    # add dirichlet noise to the root node? (page 14, Mastering Chess and Shogi by self play... --> configuration)\n",
        "    children = root_node.children\n",
        "    assert root_node.children != [], \"No children, cannot choose move\"\n",
        "    p = [child.calculate_move_probability(num_move) for child in children] \n",
        "    p_norm = [i/sum(p) for i in p] # normalize probabilities\n",
        "        \n",
        "    root_node = np.random.choice(\n",
        "        children, \n",
        "        p = p_norm  # choose the child proportionally to the number of times it has been visited (exponentiated by a temperature parameter)\n",
        "    ) \n",
        "    root_node.parent = None # To detach the subtree and restart with the next move search\n",
        "\n",
        "    return root_node\n",
        "\n",
        "\n",
        "@ray.remote(num_returns=4, max_calls=1) # max_calls = 1 is to avoid memory leaking from tensorflow, to release the unused memroy\n",
        "def complete_game(model, \n",
        "                  starting_fen=None, \n",
        "                  max_depth=conf.MAX_DEPTH, \n",
        "                  num_restarts=conf.NUM_RESTARTS\n",
        "                  ):\n",
        "\n",
        "    board = chess.Board()\n",
        "    if starting_fen != None:\n",
        "        board.set_fen(starting_fen)\n",
        "    board_history = [board.fen()[:-6]]                           # we remove the \"en passant\", \"halfmove clock\" and \"fullmove number\" from the fen --> position will be identical even if those values differ\n",
        "    \n",
        "    root_node = MyNode(\n",
        "        \"Start\",                                                     # no name needed for initial position\n",
        "        board = board,\n",
        "        board_history = board_history,\n",
        "        planes = utils.update_planes(None, board, board_history),    # start from empty planes and fill them (usually you need previous planes to fill them)\n",
        "        action_value=0,\n",
        "        visit_count=0,\n",
        "        is_finish_position = False\n",
        "        )\n",
        "    \n",
        "    match_planes = []\n",
        "    match_legal_moves = []\n",
        "    match_policy = []\n",
        "    move_counter = 0\n",
        "\n",
        "    # while not root_node.board.is_game_over(claim_draw=True) and root_node.board.fullmove_number <= conf.MAX_MOVE_COUNT:\n",
        "    while not root_node.board.is_game_over(claim_draw=True) and move_counter < conf.MAX_MOVE_COUNT:\n",
        "        move_counter += 1\n",
        "        root_node = MTCS(model, root_node, max_depth = max_depth, num_restarts=num_restarts)                            # though the root node you can access all the tree\n",
        "\n",
        "        match_planes.append(root_node.planes)                                                                                   # 8x8x113\n",
        "        match_legal_moves.append(utils.scatter_idxs(utils.mask_moves_flatten(root_node.board.legal_moves)))\n",
        "        root_node = choose_move(root_node, num_move=move_counter)                                                                                      \n",
        "        match_policy.append(utils.mask_moves_flatten([chess.Move.from_uci(root_node.name)])[0])                                         # appends JUST AN INDEX\n",
        "\n",
        "    if move_counter >= conf.MAX_MOVE_COUNT:\n",
        "        outcome = utils.outcome(\"1/2-1/2\")\n",
        "    else:\n",
        "        outcome = utils.outcome(root_node.board.outcome(claim_draw=True).result())\n",
        "    \n",
        "    return match_planes, match_legal_moves, match_policy, outcome\n",
        "\n",
        "\n",
        "def gradient_application(planes, legal_moves_input, y_policy, y_value, model, metric):\n",
        "    with tf.GradientTape() as tape:\n",
        "        policy_logits, value = model([planes, legal_moves_input])\n",
        "        # print(\"y\", y_policy, np.shape(y_policy), type(y_policy))\n",
        "        # print(\"policy_logits\", np.average(np.abs(policy_logits)), policy_logits[0][int(y_policy[0])])\n",
        "        # print(\"value_logits\", np.shape(value), type(value), np.average(value))\n",
        "        policy_loss_value = conf.LOSS_FN_POLICY(y_policy, policy_logits)\n",
        "        value_loss_value = conf.LOSS_FN_VALUE(y_value, value)\n",
        "        # print(policy_loss_value, value_loss_value, sum(model.losses))\n",
        "        loss = policy_loss_value + value_loss_value + sum(model.losses) # to add regularization loss\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_weights)\n",
        "    # print(\"avg weight\", np.average([np.average(np.abs(w)) for w in model.trainable_weights]))\n",
        "    # print(\"avg grad\", np.average([np.average(np.abs(g)) for g in grads]))\n",
        "    grads = [tf.clip_by_norm(g, 1) for g in grads] # avg weight\n",
        "    model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    metric.update_state(y_policy, policy_logits)\n",
        "\n",
        "    return policy_loss_value, value_loss_value, loss\n",
        "\n",
        "\n",
        "def train_loop( model_creation_fn,\n",
        "                total_steps = conf.TOTAL_STEPS,\n",
        "                steps_per_checkpoint = conf.STEPS_PER_EVAL_CKPT,\n",
        "                parallel_games = conf.NUM_PARALLEL_GAMES,\n",
        "                consec_train_steps = conf.NUM_TRAINING_STEPS,\n",
        "                batch_size = conf.SELF_PLAY_BATCH,\n",
        "                restart_from = 0,\n",
        "                max_depth_MCTS = conf.MAX_DEPTH,\n",
        "                num_restarts_MCTS = conf.NUM_RESTARTS\n",
        "                ):\n",
        "\n",
        "    custom_objects = {\"LogitsMaskToSoftmax\": LogitsMaskToSoftmax}\n",
        "    fixed_model = model_creation_fn()\n",
        "    config = fixed_model.get_config()\n",
        "    print(fixed_model.optimizer.get_weights())\n",
        "    utils.get_and_save_optimizer_weights(fixed_model)\n",
        "\n",
        "    if restart_from == 0:        \n",
        "        updating_model = model_creation_fn()\n",
        "        updating_model.save(conf.PATH_UPDATING_MODEL)\n",
        "        updating_model.save(conf.PATH_CKPT_FOR_EVAL.format(0), save_traces=False)\n",
        "        # TODO: should also delete files from /logs and /model_checkpoint ???\n",
        "    elif restart_from == \"last_ckpt\":\n",
        "        with tf.keras.utils.custom_object_scope(custom_objects):\n",
        "            updating_model = tf.keras.Model.from_config(config)\n",
        "            utils.set_optimizer_weights(updating_model)\n",
        "        updating_model.load_weights(conf.PATH_UPDATING_MODEL)\n",
        "\n",
        "    actual_steps = total_steps-restart_from\n",
        "    print(\"Total loops = {}\".format(int(actual_steps/consec_train_steps)))\n",
        "    print(\"Total games that will be played = {}\".format(int(actual_steps/consec_train_steps*parallel_games)))\n",
        "    print(\"Total samples that will be used = {}\".format(batch_size*actual_steps))\n",
        "    print(\"Total checkpoints that will be created = {}\".format(int(actual_steps/steps_per_checkpoint)))\n",
        "    \n",
        "    steps = restart_from\n",
        "    steps_from_last_ckpt = 0\n",
        "    loss_updater = utils.LossUpdater()\n",
        "\n",
        "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    train_log_dir = 'logs/' + current_time\n",
        "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "\n",
        "    updating_model.optimizer.iterations.assign(restart_from)\n",
        "    metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    tot_moves = 0\n",
        "    tot_games = 0\n",
        "\n",
        "    while steps < total_steps:\n",
        "        print(ray.available_resources())\n",
        "        steps += consec_train_steps\n",
        "        steps_from_last_ckpt += consec_train_steps\n",
        "        \n",
        "        tic = time()\n",
        "        starting_positions = dataset_train.take(parallel_games)\n",
        "        \n",
        "        game_ids = []\n",
        "\n",
        "        for position in starting_positions:\n",
        "        # for position in (pbar := tqdm(starting_positions)):\n",
        "            # pbar.set_description(\"Initializing ray tasks\")\n",
        "            game_ids.append(\n",
        "                complete_game.remote(\n",
        "                    fixed_model, \n",
        "                    starting_fen=position.numpy().decode(\"utf8\"), \n",
        "                    max_depth=max_depth_MCTS, \n",
        "                    num_restarts=num_restarts_MCTS\n",
        "                )\n",
        "            )\n",
        "\n",
        "        round_moves = 0\n",
        "        for id_ in game_ids:\n",
        "        # for id_ in (pbar := tqdm(game_ids)):\n",
        "        #     pbar.set_description(\"Retrieving parallel games\")\n",
        "            planes, legal_moves, moves, outcome = ray.get(id_)\n",
        "            round_moves += exp_buffer.push(planes, legal_moves, moves, outcome)\n",
        "            # delete the reference to \"ray.get\"\n",
        "            del planes, legal_moves, moves, outcome\n",
        "        \n",
        "        # delete the reference to \"ray.put\" or \"ray.remote\"\n",
        "        del game_ids # to decrease / avoid memory leaks caused by ray in its object_store_memory\n",
        "\n",
        "        tot_moves += round_moves\n",
        "        tot_games += parallel_games\n",
        "        print(\"Finished {} parallel games in {:.2f}s, stacked {} moves in exp buffer (tot {}), the learning step will consume {} moves\".format(parallel_games, time()-tic, round_moves, exp_buffer.filled_up, consec_train_steps*batch_size))\n",
        "        print(\"Decisive result percentage = {:.2f}%\".format(exp_buffer.get_percentage_decisive_games()))\n",
        "        print(\"On average, the same move will be passed through the network {:.2f} times\".format(consec_train_steps*batch_size/round_moves))\n",
        "        print(\"The avg length of a game is {:.2f}\".format(tot_moves/tot_games))\n",
        "        tic = time()\n",
        "\n",
        "        for _ in range(consec_train_steps):\n",
        "            planes_batch, legal_moves_input_batch, moves_batch, outcome_batch = exp_buffer.sample(batch_size)\n",
        "            \n",
        "            policy_loss_value, value_loss_value, loss = gradient_application(\n",
        "                planes_batch,\n",
        "                legal_moves_input_batch, \n",
        "                moves_batch, \n",
        "                outcome_batch, \n",
        "                updating_model,\n",
        "                metric)\n",
        "\n",
        "            loss_updater.update(policy_loss_value, value_loss_value, loss)\n",
        "\n",
        "        # print(\"Finished {} train steps in {}s\".format(consec_train_steps, time()-tic, tot_moves))\n",
        "        p_loss, v_loss, tot_loss = loss_updater.get_losses()\n",
        "        p_metric = metric.result()\n",
        "        # print(\"Finished training steps --> Policy loss {:.5f} - value loss {:.5f} - loss {:.5f} - policy_accuracy {:.5f}\".format(p_loss, v_loss, tot_loss, p_metric))\n",
        "        loss_updater.reset_state()\n",
        "        metric.reset_states()\n",
        "        \n",
        "        with train_summary_writer.as_default():\n",
        "            tf.summary.scalar('policy_loss', p_loss, step=steps)\n",
        "            tf.summary.scalar('value_loss', v_loss, step=steps)\n",
        "            tf.summary.scalar('L2_loss', tot_loss-p_loss-v_loss, step=steps)\n",
        "            tf.summary.scalar('total_loss', tot_loss, step=steps)\n",
        "            tf.summary.scalar('policy_accuracy', p_metric, step=steps)\n",
        "            tf.summary.scalar('lr', updating_model.optimizer.lr(updating_model.optimizer.iterations), steps)\n",
        "            \n",
        "            for layer in updating_model.layers:\n",
        "                for i, weight in enumerate(layer.trainable_weights):\n",
        "                    if i==0:\n",
        "                        kind = \"_kernel\"\n",
        "                    else:\n",
        "                        kind = \"_bias\"\n",
        "                    tf.summary.histogram(layer.name+kind, weight, steps)\n",
        "        \n",
        "        updating_model.save(conf.PATH_UPDATING_MODEL, save_traces=False) # should decrease saving time, since we don't have custome layers/models\n",
        "        fixed_model = tf.keras.models.load_model(conf.PATH_UPDATING_MODEL) # the UPDATING MODEL is loaded into the fixed one\n",
        "        \n",
        "        if steps_from_last_ckpt >= steps_per_checkpoint:\n",
        "            steps_from_last_ckpt = 0\n",
        "            print(\"Saving checkpoint at step {}\".format(steps))\n",
        "            updating_model.save(conf.PATH_CKPT_FOR_EVAL.format(steps), save_traces=False)\n",
        "\n",
        "\n",
        "# train_loop(\n",
        "#     create_model, \n",
        "#     total_steps=conf.TOTAL_STEPS,\n",
        "#     parallel_games=conf.NUM_PARALLEL_GAMES,\n",
        "#     consec_train_steps=conf.NUM_TRAINING_STEPS,\n",
        "#     steps_per_checkpoint=conf.STEPS_PER_EVAL_CKPT,\n",
        "#     batch_size=conf.SELF_PLAY_BATCH,\n",
        "#     restart_from=0)\n",
        "\n",
        "train_loop(\n",
        "    create_model, \n",
        "    total_steps=300,\n",
        "    parallel_games=1,\n",
        "    consec_train_steps=10,\n",
        "    steps_per_checkpoint=20,\n",
        "    batch_size=8,\n",
        "    restart_from=0)\n",
        "\n",
        "# model = create_model()\n",
        "# model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "q6SDrlqFzlzM",
        "outputId": "2f6a8438-dfa3-4ba0-c62e-8293b17a3b0e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-da0bee0ecc2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0msteps_per_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     restart_from=0)\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;31m# model = create_model()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-da0bee0ecc2d>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(model_creation_fn, total_steps, steps_per_checkpoint, parallel_games, consec_train_steps, batch_size, restart_from, max_depth_MCTS, num_restarts_MCTS)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfixed_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixed_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m     \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_and_save_optimizer_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixed_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrestart_from\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ChessBreaker/utils.py\u001b[0m in \u001b[0;36mget_and_save_optimizer_weights\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZER_W_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_and_set_optimizer_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rhn-eORD4pI6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}