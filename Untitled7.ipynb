{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j2iWAxgCHN7z"
      },
      "outputs": [],
      "source": [
        "username = 'MarcelloCeresini'\n",
        "repository = 'ChessBreaker'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YG5cfeYvHhdU",
        "outputId": "83f1d0f6-6925-4b26-a582-8485ff66edde"
      },
      "outputs": [],
      "source": [
        "# COLAB ONLY CELLS\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    !nvidia-smi             # Check which GPU has been chosen for us\n",
        "    !rm -rf logs\n",
        "    #from google.colab import drive\n",
        "    #drive.mount('/content/drive')\n",
        "    #%cd /content/drive/MyDrive/GitHub/\n",
        "    !git clone https://github.com/{username}/{repository}.git\n",
        "    %cd {repository}\n",
        "    %ls\n",
        "    !pip3 install anytree\n",
        "except:\n",
        "    IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5qDwSWUnXcjn"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-08-01 19:08:29.039104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-08-01 19:08:29.066118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-08-01 19:08:29.066308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-08-01 19:08:29.067147: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2022-08-01 19:08:29.068983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-08-01 19:08:29.069169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-08-01 19:08:29.069481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-08-01 19:08:29.518508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-08-01 19:08:29.518673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-08-01 19:08:29.518795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-08-01 19:08:29.518913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3245 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import chess\n",
        "from anytree import Node\n",
        "from time import time\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from queue import Queue\n",
        "from collections import deque\n",
        "\n",
        "from numpy.random import default_rng\n",
        "rng = default_rng()\n",
        "\n",
        "import utils\n",
        "from utils import plane_dict, Config, x_y_from_position\n",
        "from model import create_model_v2\n",
        "\n",
        "conf = Config()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yv2zcMiUh9bU"
      },
      "outputs": [],
      "source": [
        "class MyNode(Node): # subclassing Node from Anytree to add some methods\n",
        "\n",
        "    def update_action_value(self, new_action_value):                                                        # used during backtracking to update action value if the simulation reached the end through that node\n",
        "        self.action_value += (new_action_value-self.action_value)/(self.visit_count+1)                      # simply the mean value, but computed iteratively\n",
        "\n",
        "    def calculate_upper_confidence_bound(self, num_total_iterations=1):                                     # Q + U --> U proportional to P/(1+N) --> parameter decides exploration vs. exploitation\n",
        "        new_UCF = self.action_value + conf.expl_param(num_total_iterations)*self.prior/(1+self.visit_count)\n",
        "        return new_UCF\n",
        "\n",
        "    def calculate_move_probability(self, num_move=1):                                           # N^(1/tau) --> tau is a temperature parameter (exploration vs. exploitation)\n",
        "        return self.visit_count**(1/conf.temp_param(num_move))\n",
        "\n",
        "\n",
        "def MTCS(model, root_node, max_depth, num_restarts):\n",
        "    '''\n",
        "        The search descends until it finds a leaf to be evalued, then restarts until it gathers a batch of evaluations, then expands all the nodes in the batch\n",
        "        If the maximum depth is reached, the algorithm then backpropagates the action value of the reached node back to the root of the tree\n",
        "        Exploration incentivised by the decrease of action value / upper confidence bound with each visit to the node\n",
        "        Exploitation incentivised by the two parameters temp_param and expl_param (respectively to choose almost surely the most probable move, and to focus more on the action value rather than the prior of the node)\n",
        "    '''\n",
        "    evaluation_counter = 0\n",
        "    INIT_ROOT = root_node\n",
        "    nodes_to_visit = Queue()\n",
        "    # number of times to explore up until max_depth\n",
        "    i = 0\n",
        "\n",
        "    leaf_node_batch = []\n",
        "    legal_moves_batch = []\n",
        "    \n",
        "    while i < num_restarts:\n",
        "        if nodes_to_visit.empty():\n",
        "            root_node = INIT_ROOT\n",
        "            i+=1\n",
        "        else:\n",
        "            root_node = nodes_to_visit.get_nowait()\n",
        "        # print(i, root_node.name, root_node.depth, root_node.visit_count)\n",
        "        \n",
        "        step_down = True\n",
        "        control_counter = 0\n",
        "        while step_down:\n",
        "            control_counter+=1\n",
        "            if control_counter > 2*max_depth: \n",
        "                print(\"stuck in loop, leaving\")\n",
        "                break # bigger margin, but if it is stuk in a loop for some reason, at least it leaves\n",
        "            \n",
        "            # assert root_node.depth >= 0 and root_node.depth <= max_depth, \"depth is wrong\"\n",
        "            if root_node.is_leaf and not root_node.is_finish_position:                                                                           # if it's leaf --> need to pass the position (planes) through the model, to get priors (action_values) and outcome (state_value)\n",
        "                step_down = False\n",
        "\n",
        "                if len(root_node.siblings) > 0:         # this part is to try and avoid batching the same node twice (so we evaluate a random sibling instead)\n",
        "                    leaf_node_list = [node.board_history for node in leaf_node_batch]\n",
        "                    if root_node.board_history in leaf_node_list:\n",
        "                        siblings_list = list(root_node.siblings)\n",
        "                        random_sibling = np.random.choice(siblings_list)\n",
        "                        while random_sibling.board_history in leaf_node_list and len(siblings_list)>1:\n",
        "                            siblings_list.remove(random_sibling) #do it with np.random in a range, and POP instead of remove\n",
        "                            random_sibling = np.random.choice(siblings_list)\n",
        "                        \n",
        "                        root_node = random_sibling\n",
        "                        \n",
        "                # important! save legal moves AFTER choosing root_node\n",
        "                legal_moves = list(root_node.board.legal_moves)\n",
        "\n",
        "                if len(legal_moves) == 0:\n",
        "                    step_down = False\n",
        "                    final_outcome = root_node.board.outcome(claim_draw=True)\n",
        "\n",
        "                    if final_outcome != None:\n",
        "                        root_node.is_finish_position = True\n",
        "                        if final_outcome.winner == None:\n",
        "                            root_node.action_value = 0\n",
        "                        else:\n",
        "                            root_node.action_value = int(final_outcome.winner)*2-1\n",
        "                    else:\n",
        "                        print(\"Something's wrong\")\n",
        "                else:\n",
        "                    leaf_node_batch.append(root_node)\n",
        "                    legal_moves_batch.append(legal_moves)\n",
        "                \n",
        "                root_node.visit_count += 1\n",
        "\n",
        "                # print(\"to be evaluated\", \"d\", root_node.depth, \"vc\", root_node.visit_count, \"name\", root_node.name)\n",
        "\n",
        "                if len(leaf_node_batch) == conf.BATCH_DIM or root_node == INIT_ROOT:\n",
        "                    # in order to avoid creating multiple times the children of the same node, we only keep unique values\n",
        "                    if len(set(leaf_node_batch)) < conf.BATCH_DIM:\n",
        "                        leaf_node_batch, legal_moves_batch = utils.reduce_repetitions(leaf_node_batch, legal_moves_batch)                    \n",
        "                    \n",
        "                    plane_list = [root_node.planes for root_node in leaf_node_batch]\n",
        "                    # 0.0032072067260742188\n",
        "                    # 7.05718994140625e-05\n",
        "                    \n",
        "                    planes = np.stack(plane_list)\n",
        "\n",
        "                    full_moves_batch, outcome_batch = model(planes)\n",
        "                    evaluation_counter+=1\n",
        "\n",
        "                    full_moves_batch_np = full_moves_batch.numpy()\n",
        "                    # print(np.shape(full_moves_batch_np[0]))\n",
        "                    outcome_batch_np = outcome_batch.numpy()\n",
        "\n",
        "                    if np.shape(full_moves_batch_np)[0] != 1:\n",
        "                        full_moves_batch_np = np.moveaxis(full_moves_batch.numpy(), 0, 0)\n",
        "                        outcome_batch_np = np.moveaxis(outcome_batch.numpy(), 0, 0)\n",
        "                    \n",
        "                    for root_node, full_moves, outcome, legal_moves in zip(leaf_node_batch, full_moves_batch_np, outcome_batch_np, legal_moves_batch):\n",
        "                        nodes_to_visit.put_nowait(root_node)\n",
        "                        \n",
        "                        mask_idx = utils.mask_moves(legal_moves)\n",
        "                        priors = [full_moves[idx[0], idx[1], idx[2]] for idx in mask_idx]                        # boolean mask returns a tensor of only the values that were masked (as a list let's say)\n",
        "                        # 0.006434917449951172\n",
        "                        # 1.3113021850585938e-05\n",
        "                        root_node.action_value = outcome    \n",
        "                        \n",
        "                        if root_node == INIT_ROOT:  # increase exploration at the root, since if the network is not good it will not make good starting choices\n",
        "                            dir_noise = rng.dirichlet([conf.ALPHA_DIRICHLET]*len(priors))\n",
        "                            priors = (1-conf.EPS_NOISE)*priors + conf.EPS_NOISE*dir_noise\n",
        "\n",
        "                        for move, prior in zip(legal_moves, priors):                                                # creating children\n",
        "\n",
        "                            root_board_fen = root_node.board.fen()\n",
        "                            new_board = chess.Board()\n",
        "                            new_board.set_fen(root_board_fen)\n",
        "                            new_board.push(move)\n",
        "                                                        \n",
        "                            new_board_history = root_node.board_history.copy()                                      # and board history! (copy because list are pointers)\n",
        "                            new_board_history.append(new_board.fen()[:-6])\n",
        "\n",
        "                            planes = utils.update_planes(root_node.planes, new_board, new_board_history)\n",
        "                            \n",
        "                            MyNode(\n",
        "                                move.uci(), \n",
        "                                parent = root_node,                                                                 # very important to build the tree\n",
        "                                prior = prior,                                                                      # prior is the \"initial\" state_value of a node\n",
        "                                visit_count = 0,                                                                    # initialize visit_count to 0\n",
        "                                action_value = 0,\n",
        "                                is_finish_position = False,\n",
        "                                board = new_board, \n",
        "                                board_history = new_board_history,                                                  \n",
        "                                planes = planes             # update the planes --> each node stores its input planes!\n",
        "                            )\n",
        "\n",
        "                    leaf_node_batch = []\n",
        "                    legal_moves_batch = []\n",
        "\n",
        "            else: # if it does not need to be evalued because it already has children \n",
        "                if root_node.depth < max_depth and not root_node.is_finish_position:                                # if we are normally descending\n",
        "                    # print(\"choosing point\", \"d\", root_node.depth, \"vc\", root_node.visit_count, \"name\", root_node.name)\n",
        "                    children = root_node.children                                                               # get all the children (always != [])\n",
        "                    \n",
        "                    values = [child.calculate_upper_confidence_bound() for child in children]\n",
        "                    root_node = children[np.argmax(values)]\n",
        "                    root_node.visit_count += 1                                                                  # add 1 to the visit count of the chosen child\n",
        "                    # print(\"chosen node\", \"d\", root_node.depth, \"vc\", root_node.visit_count, \"name\", root_node.name)\n",
        "                else:\n",
        "                    step_down = False                                # it will leave the while, max depth is reached\n",
        "                    # print(\"final leaf\", \"d\", root_node.depth, \"vc\", root_node.visit_count, \"name\", root_node.name, root_node.calculate_upper_confidence_bound())\n",
        "                    outcome = root_node.action_value    # needed for when depth=max_depth AND NOT LEAF (that means, already visited leaf) --> don't REDO the evaluation, it would give the same result, simply copy it from before\n",
        "                    # barckpropagation of action value through the tree\n",
        "                    while root_node.depth > 0:\n",
        "                        # root node should be an already evalued leaf, at max depth (so OUTCOME has been set)\n",
        "                        # assert root_node.depth > 0 and root_node.depth <= max_depth, \"depth is wrong\"\n",
        "                        root_node = root_node.parent\n",
        "                        root_node.update_action_value(outcome)\n",
        "\n",
        "    return INIT_ROOT, evaluation_counter\n",
        "\n",
        "\n",
        "def choose_move(root_node, num_move):\n",
        "    # add dirichlet noise to the root node? (page 14, Mastering Chess and Shogi by self play... --> configuration)\n",
        "    children = root_node.children\n",
        "    assert root_node.children != [], \"No children, cannot choose move\"\n",
        "    p = [child.calculate_move_probability(num_move) for child in children] \n",
        "    p_norm = [i/sum(p) for i in p] # normalize probabilities\n",
        "\n",
        "    root_node = np.random.choice(\n",
        "        children, \n",
        "        p = p_norm  # choose the child proportionally to the number of times it has been visited (exponentiated by a temperature parameter)\n",
        "    ) \n",
        "    root_node.parent = None # To detach the subtree and restart with the next move search\n",
        "\n",
        "    return root_node\n",
        "\n",
        "\n",
        "def complete_game(model, starting_fen=None, max_depth=conf.MAX_DEPTH, num_restarts=conf.NUM_RESTARTS):\n",
        "    debugging = False\n",
        "    board = chess.Board()\n",
        "    if starting_fen != None:\n",
        "        print(starting_fen)\n",
        "        board.set_fen(starting_fen)\n",
        "    board_history = [board.fen()[:-6]]                           # we remove the \"en passant\", \"halfmove clock\" and \"fullmove number\" from the fen --> position will be identical even if those values differ\n",
        "    \n",
        "    root_node = MyNode(\n",
        "        \"Start\",                                                     # no name needed for initial position\n",
        "        board = board,\n",
        "        board_history = board_history,\n",
        "        planes = utils.update_planes(None, board, board_history),    # start from empty planes and fill them (usually you need previous planes to fill them)\n",
        "        action_value=0,\n",
        "        visit_count=0,\n",
        "        is_finish_position = False\n",
        "        )\n",
        "\n",
        "    if debugging:\n",
        "        move_list = []\n",
        "        results = []\n",
        "    \n",
        "    match_planes = []\n",
        "    match_policy = []\n",
        "    move_counter = 0\n",
        "    # while not root_node.board.is_game_over(claim_draw=True) and root_node.board.fullmove_number <= conf.MAX_MOVE_COUNT:\n",
        "    while not root_node.board.is_game_over(claim_draw=True) and move_counter < conf.MAX_MOVE_COUNT:\n",
        "        move_counter += 1\n",
        "        tic = time()\n",
        "        root_node, eval_c = MTCS(model, root_node, max_depth = max_depth, num_restarts=num_restarts)                            # though the root node you can access all the tree\n",
        "\n",
        "        match_planes.append(root_node.planes)                                                                                   # 8x8x113\n",
        "        root_node = choose_move(root_node, num_move=move_counter)                                                                                      \n",
        "        match_policy.append(utils.mask_moves([chess.Move.from_uci(root_node.name)])[0])                                         # appends JUST AN INDEX\n",
        "\n",
        "        if debugging:\n",
        "            ###### only for debugging ######\n",
        "            move_list.append(chess.Move.from_uci(root_node.name))\n",
        "            results.append((\n",
        "                int(root_node.planes[0,0,conf.REPEATED_PLANES+conf.OLD_PLANES_TO_KEEP+1]),  # it is the fullmove number\n",
        "                root_node.visit_count,\n",
        "                eval_c,\n",
        "                time()-tic,\n",
        "                root_node.action_value))\n",
        "            \n",
        "            print(move_counter, time()-tic)\n",
        "            ################################\n",
        "\n",
        "    if move_counter >= conf.MAX_MOVE_COUNT:\n",
        "        outcome = utils.outcome(\"1/2-1/2\")\n",
        "    else:\n",
        "        outcome = utils.outcome(root_node.board.outcome(claim_draw=True).result())\n",
        "\n",
        "    # TODO: try if \"match_policy\" is a problem, because it's a list\n",
        "    if debugging:\n",
        "        return move_list, outcome, results, match_planes, match_policy                             # only needed for the program are the match_planes (input for learning) and the outcome/match_policy (loss)\n",
        "    \n",
        "    return match_planes, match_policy, outcome\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uNRD2BohKG2g"
      },
      "outputs": [],
      "source": [
        "# model = ResNet()\n",
        "# DUMMY_INPUT = tf.stack([tf.zeros([*conf.BOARD_SHAPE, conf.TOTAL_PLANES])]*8, axis = 0)\n",
        "# print(tf.shape(DUMMY_INPUT))\n",
        "# fm, ac = model(DUMMY_INPUT)\n",
        "# print(\"full moves shape\", fm.shape)\n",
        "# print(\"action values shape\", ac.shape)\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1pHHpicUKG2h"
      },
      "outputs": [],
      "source": [
        "# model = create_model_v2()\n",
        "# res_dict = {}\n",
        "# games = []\n",
        "# outcomes = []\n",
        "# for depth in [8, 6, 4, 2]:\n",
        "#     for n_rep in [100, 80, 60, 40, 30, 20, 10, 5]:\n",
        "#         tic = time()\n",
        "#         moves, outcome, results, _, _ = complete_game(model, max_depth=depth, num_restarts=n_rep)\n",
        "#         res_dict[(depth, n_rep, \"eval\")] = np.average([res[2] for res in results])\n",
        "#         res_dict[(depth, n_rep, \"time\")] = np.average([res[3] for res in results])\n",
        "#         games.append(moves)\n",
        "#         outcomes.append(outcome)\n",
        "#         print(depth, \"\\t\", n_rep, \"\\t\", time()-tic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8vRhjzdqR-4e",
        "outputId": "8a2c0e07-4371-4528-d02d-01fe8d012679"
      },
      "outputs": [],
      "source": [
        "# reps = [5, 10, 20, 30, 40, 60, 80, 100]\n",
        "# depths = [2, 4, 6, 8]\n",
        "\n",
        "# %matplotlib inline\n",
        "\n",
        "# [plt.scatter(reps, [res_dict[(depth, rep, \"eval\")] for rep in reps]) for depth in depths]\n",
        "\n",
        "# plt.legend(depths)\n",
        "# plt.title(\"Evaluations vs. repetitions\")\n",
        "# plt.xlabel(\"Repetitions\")\n",
        "# plt.ylabel(\"Total evaluations\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "k1HAFM0ZR-4f",
        "outputId": "1a7569a8-90bb-4bc1-e0a9-7ac7cdd1fae1"
      },
      "outputs": [],
      "source": [
        "# %matplotlib inline\n",
        "\n",
        "# [plt.scatter(reps, [res_dict[(depth, rep, \"time\")] for rep in reps]) for depth in depths]\n",
        "# # plt.scatter(reps, times)\n",
        "\n",
        "# plt.legend(depths)\n",
        "# plt.title(\"Time vs. repetitions\")\n",
        "# plt.xlabel(\"Repetitions\")\n",
        "# plt.ylabel(\"Mean time per move\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Vg2o4fMGR-4h"
      },
      "outputs": [],
      "source": [
        "# import json, codecs\n",
        "\n",
        "# Zl = [[res_dict[(depth, rep, \"time\")] for rep in reps] for depth in depths]\n",
        "\n",
        "# X, Y = np.meshgrid(reps, depths)\n",
        "# # Z_time = np.stack([\n",
        "# #     times\n",
        "# # ])\n",
        "\n",
        "\n",
        "# Xl = X.tolist() # nested lists with same data, indices\n",
        "# Yl = Y.tolist() # nested lists with same data, indices\n",
        "# # Zl = Z_time.tolist() # nested lists with same data, indices\n",
        "\n",
        "# file_path_X = \"data/chartX.json\" ## your path variable\n",
        "# file_path_Y = \"data/chartY.json\" ## your path variable\n",
        "# file_path_Z = \"data/chartZ.json\" ## your path variable\n",
        "\n",
        "# json.dump(Xl, codecs.open(file_path_X, 'w', encoding='utf-8'), \n",
        "#           separators=(',', ':'), \n",
        "#           sort_keys=True, \n",
        "#           indent=4) ### this saves the array in .json format\n",
        "\n",
        "# json.dump(Yl, codecs.open(file_path_Y, 'w', encoding='utf-8'), \n",
        "#           separators=(',', ':'), \n",
        "#           sort_keys=True, \n",
        "#           indent=4) ### this saves the array in .json format\n",
        "\n",
        "# json.dump(Zl, codecs.open(file_path_Z, 'w', encoding='utf-8'), \n",
        "#           separators=(',', ':'), \n",
        "#           sort_keys=True, \n",
        "#           indent=4) ### this saves the array in .json format\n",
        "\n",
        "# # fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
        "# # surf = ax.plot_surface(X, Y, Z_time, linewidth=0, antialiased=False)\n",
        "\n",
        "# # plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-08-01 19:08:30.985591: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8302\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 8, 8, 119)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 8, 8, 256)    274432      ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 8, 8, 256)   1024        ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 8, 8, 256)    0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 8, 8, 256)    590080      ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 8, 8, 256)   1024        ['conv2d_1[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 8, 8, 256)    0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 8, 8, 256)    590080      ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 8, 8, 256)   1024        ['conv2d_2[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 8, 8, 256)    0           ['batch_normalization_2[0][0]',  \n",
            "                                                                  'activation[0][0]']             \n",
            "                                                                                                  \n",
            " activation_resblock_0 (Activat  (None, 8, 8, 256)   0           ['add[0][0]']                    \n",
            " ion)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 8, 8, 256)    590080      ['activation_resblock_0[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 8, 8, 256)   1024        ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 8, 8, 256)    0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 8, 8, 256)    590080      ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 8, 8, 256)   1024        ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 8, 8, 256)    0           ['batch_normalization_4[0][0]',  \n",
            "                                                                  'activation_resblock_0[0][0]']  \n",
            "                                                                                                  \n",
            " activation_resblock_1 (Activat  (None, 8, 8, 256)   0           ['add_1[0][0]']                  \n",
            " ion)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 8, 8, 256)    590080      ['activation_resblock_1[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 8, 8, 256)   1024        ['conv2d_5[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 8, 8, 256)    0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 8, 8, 256)    590080      ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 8, 8, 256)   1024        ['conv2d_6[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 8, 8, 256)    0           ['batch_normalization_6[0][0]',  \n",
            "                                                                  'activation_resblock_1[0][0]']  \n",
            "                                                                                                  \n",
            " activation_resblock_2 (Activat  (None, 8, 8, 256)   0           ['add_2[0][0]']                  \n",
            " ion)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 8, 8, 256)    590080      ['activation_resblock_2[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 8, 8, 256)   1024        ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 8, 8, 256)    0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 8, 8, 256)    590080      ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 8, 8, 256)   1024        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 8, 8, 256)    0           ['batch_normalization_8[0][0]',  \n",
            "                                                                  'activation_resblock_2[0][0]']  \n",
            "                                                                                                  \n",
            " activation_resblock_3 (Activat  (None, 8, 8, 256)   0           ['add_3[0][0]']                  \n",
            " ion)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 8, 8, 256)    590080      ['activation_resblock_3[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 8, 8, 256)   1024        ['conv2d_9[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 8, 8, 256)    0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 8, 8, 256)    590080      ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_10[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 8, 8, 256)    0           ['batch_normalization_10[0][0]', \n",
            "                                                                  'activation_resblock_3[0][0]']  \n",
            "                                                                                                  \n",
            " activation_resblock_4 (Activat  (None, 8, 8, 256)   0           ['add_4[0][0]']                  \n",
            " ion)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 8, 8, 256)    590080      ['activation_resblock_4[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_11[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 8, 8, 256)    0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 8, 8, 256)    590080      ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_12[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 8, 8, 256)    0           ['batch_normalization_12[0][0]', \n",
            "                                                                  'activation_resblock_4[0][0]']  \n",
            "                                                                                                  \n",
            " activation_resblock_5 (Activat  (None, 8, 8, 256)   0           ['add_5[0][0]']                  \n",
            " ion)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 8, 8, 256)    590080      ['activation_resblock_5[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_13[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 8, 8, 256)    0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 8, 8, 256)    590080      ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_14[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 8, 8, 256)    0           ['batch_normalization_14[0][0]', \n",
            "                                                                  'activation_resblock_5[0][0]']  \n",
            "                                                                                                  \n",
            " activation_resblock_6 (Activat  (None, 8, 8, 256)   0           ['add_6[0][0]']                  \n",
            " ion)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 8, 8, 256)    590080      ['activation_resblock_6[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 8, 8, 256)    0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 8, 8, 256)    590080      ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 8, 8, 256)    0           ['batch_normalization_16[0][0]', \n",
            "                                                                  'activation_resblock_6[0][0]']  \n",
            "                                                                                                  \n",
            " activation_resblock_7 (Activat  (None, 8, 8, 256)   0           ['add_7[0][0]']                  \n",
            " ion)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 8, 8, 256)    590080      ['activation_resblock_7[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_17[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 8, 8, 256)    0           ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 8, 8, 256)    590080      ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_18[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 8, 8, 256)    0           ['batch_normalization_18[0][0]', \n",
            "                                                                  'activation_resblock_7[0][0]']  \n",
            "                                                                                                  \n",
            " activation_resblock_8 (Activat  (None, 8, 8, 256)   0           ['add_8[0][0]']                  \n",
            " ion)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 8, 8, 256)    590080      ['activation_resblock_8[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 8, 8, 256)    0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 8, 8, 256)    590080      ['activation_10[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_9 (Add)                    (None, 8, 8, 256)    0           ['batch_normalization_20[0][0]', \n",
            "                                                                  'activation_resblock_8[0][0]']  \n",
            "                                                                                                  \n",
            " activation_resblock_9 (Activat  (None, 8, 8, 256)   0           ['add_9[0][0]']                  \n",
            " ion)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 8, 8, 1)      257         ['activation_resblock_9[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 8, 8, 1)     4           ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 8, 8, 1)      0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 8, 8, 128)    32896       ['activation_resblock_9[0][0]']  \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 64)           0           ['activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 8, 8, 128)   512         ['conv2d_21[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 256)          16640       ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 8, 8, 128)    0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 256)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 8, 8, 73)     9417        ['activation_11[0][0]']          \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            257         ['activation_13[0][0]']          \n",
            "                                                                                                  \n",
            " policy (Flatten)               (None, 4672)         0           ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " value (Activation)             (None, 1)            0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 12,157,519\n",
            "Trainable params: 12,146,509\n",
            "Non-trainable params: 11,010\n",
            "__________________________________________________________________________________________________\n",
            "tf.Tensor(\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]], shape=(8, 4672), dtype=float32) tf.Tensor(\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]], shape=(8, 1), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "fixed_model = create_model_v2()\n",
        "\n",
        "fixed_model.compile(\n",
        "    optimizer=conf.OPTIMIZER,\n",
        "    loss={\"policy\":conf.LOSS_FN_POLICY, \"value\":conf.LOSS_FN_VALUE},\n",
        "    metrics={\"policy\":conf.METRIC_FN_POLICY}\n",
        ")\n",
        "\n",
        "DUMMY_INPUT = tf.stack([tf.zeros([*conf.BOARD_SHAPE, conf.TOTAL_PLANES])]*8, axis = 0)\n",
        "fm, ac = fixed_model(DUMMY_INPUT)\n",
        "fixed_model.summary()\n",
        "print(fm, ac)\n",
        "# fixed_model.save(conf.PATH_FIXED_MODEL)\n",
        "\n",
        "# updating_model = create_model_v2()\n",
        "# updating_model.save(conf.PATH_UPDATING_MODEL)\n",
        "\n",
        "# updating_model.compile(\n",
        "#     optimizer=conf.OPTIMIZER,\n",
        "#     loss={\"policy\":conf.LOSS_FN_POLICY, \"value\":conf.LOSS_FN_VALUE},\n",
        "#     metrics={\"policy\":conf.METRIC_FN_POLICY}\n",
        "# )\n",
        "# TODO: maybe compile them before saving?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6087219715118408\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ram://53522f61-0934-468f-a595-f99c2c37fcca/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ram://53522f61-0934-468f-a595-f99c2c37fcca/assets\n",
            "2022-08-01 19:08:39.580539: F tensorflow/stream_executor/cuda/cuda_driver.cc:152] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ram://95a9f009-d9de-4880-b7f3-d990f7371181/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ram://95a9f009-d9de-4880-b7f3-d990f7371181/assets\n",
            "2022-08-01 19:08:46.907462: F tensorflow/stream_executor/cuda/cuda_driver.cc:152] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ram://33bc07dd-c06b-4ef0-988f-f626080414a8/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ram://33bc07dd-c06b-4ef0-988f-f626080414a8/assets\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/marcello/github/ChessBreaker/Untitled7.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marcello/github/ChessBreaker/Untitled7.ipynb#ch0000017?line=46'>47</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mgames\u001b[39m\u001b[39m\"\u001b[39m, time()\u001b[39m-\u001b[39mtic)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marcello/github/ChessBreaker/Untitled7.ipynb#ch0000017?line=47'>48</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tot_num_moves, filled_up\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/marcello/github/ChessBreaker/Untitled7.ipynb#ch0000017?line=49'>50</a>\u001b[0m play_parallel_games_apply(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marcello/github/ChessBreaker/Untitled7.ipynb#ch0000017?line=50'>51</a>\u001b[0m     model\u001b[39m=\u001b[39;49mfixed_model,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marcello/github/ChessBreaker/Untitled7.ipynb#ch0000017?line=51'>52</a>\u001b[0m     num_games\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marcello/github/ChessBreaker/Untitled7.ipynb#ch0000017?line=52'>53</a>\u001b[0m     filled_up\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marcello/github/ChessBreaker/Untitled7.ipynb#ch0000017?line=53'>54</a>\u001b[0m     max_depth\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marcello/github/ChessBreaker/Untitled7.ipynb#ch0000017?line=54'>55</a>\u001b[0m     num_repetitions\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
            "\u001b[1;32m/home/marcello/github/ChessBreaker/Untitled7.ipynb Cell 11\u001b[0m in \u001b[0;36mplay_parallel_games_apply\u001b[0;34m(model, num_games, filled_up, max_depth, num_repetitions)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marcello/github/ChessBreaker/Untitled7.ipynb#ch0000017?line=28'>29</a>\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m futures:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marcello/github/ChessBreaker/Untitled7.ipynb#ch0000017?line=29'>30</a>\u001b[0m     \u001b[39mprint\u001b[39m(time()\u001b[39m-\u001b[39mtic)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/marcello/github/ChessBreaker/Untitled7.ipynb#ch0000017?line=30'>31</a>\u001b[0m     planes, moves, result \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39;49mget()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marcello/github/ChessBreaker/Untitled7.ipynb#ch0000017?line=31'>32</a>\u001b[0m     num_planes \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(planes)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marcello/github/ChessBreaker/Untitled7.ipynb#ch0000017?line=33'>34</a>\u001b[0m     to_remove \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(\u001b[39m0\u001b[39m, (filled_up \u001b[39m+\u001b[39m num_planes) \u001b[39m-\u001b[39m conf\u001b[39m.\u001b[39mMAX_BUFFER_SIZE)\n",
            "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    766\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
            "File \u001b[0;32m/usr/lib/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    557\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 558\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    559\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
            "File \u001b[0;32m/usr/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    303\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "dataset = tf.data.TextLineDataset(conf.PATH_ENDGAME_TRAIN_DATASET).shuffle(10000).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# using np.arrays because we add chunks of data and not one at a time (O(n) to move all data is actually O(n/m), with m chunk size)\n",
        "# and also we need to sample randomly batches of data, that for linked lists (like queue) is O(n*batch_zize), instead for arrays is O(batch_size)\n",
        "experience_buffer_planes = np.zeros((conf.MAX_BUFFER_SIZE, *conf.INPUT_SHAPE), dtype=conf.PLANES_DTYPE_NP)  # the bigger the better, check with some experiments\n",
        "experience_buffer_moves = np.zeros((conf.MAX_BUFFER_SIZE, 1), dtype=conf.PLANES_DTYPE_NP)   # the bigger the better, check with some experiments\n",
        "experience_buffer_outcome = np.zeros((conf.MAX_BUFFER_SIZE, 1), dtype=conf.PLANES_DTYPE_NP) # the bigger the better, check with some experiments\n",
        "# start_learning_from = 50000 # number of MOVES\n",
        "\n",
        "# idea: start learning after 50000 samples are in the queue, and randomly select them to pass them through the network, then REMOVE them from the queue\n",
        "# IN PARALLEL, keep playing games with the fixed_model to fill up the queue --> if this step is much faster (or the opposite) --> just wait a bit for the slower one, so that the queue always stays\n",
        "# between ~50k and ~100k\n",
        "\n",
        "# ideally: infinite while loop that launches in parallel two threads/processes:\n",
        "# 1) generates self-play samples (planes, (moves, outcome)) that then get COPIED (otherwise parallelism will screw everything up) and added to the queue\n",
        "# 2) randomly selects batches of 512 samples from the buffer\n",
        "\n",
        "from multiprocessing import Pool\n",
        "from numpy.random import default_rng\n",
        "rng = default_rng()\n",
        "\n",
        "def play_parallel_games_apply(model, num_games, filled_up, max_depth, num_repetitions):\n",
        "    tic = time()\n",
        "    tot_num_moves = 0\n",
        "\n",
        "    starting_positions = dataset.take(num_games)\n",
        "    with Pool() as pool: # play as many games as possible in parallel, and add them to the queue\n",
        "        futures = [pool.apply_async(complete_game, [model, position, max_depth, num_repetitions]) for position in starting_positions]\n",
        "        for f in futures:\n",
        "            print(time()-tic)\n",
        "            planes, moves, result = f.get()\n",
        "            num_planes = len(planes)\n",
        "            \n",
        "            to_remove = min(0, (filled_up + num_planes) - conf.MAX_BUFFER_SIZE)\n",
        "            to_move = conf.MAX_BUFFER_SIZE - to_remove\n",
        "            experience_buffer_planes[:to_move, ...] = experience_buffer_planes[to_remove:, ...]\n",
        "            experience_buffer_moves[:to_move, ...] = experience_buffer_moves[to_remove:, ...]\n",
        "            experience_buffer_outcome[:to_move, ...] = experience_buffer_outcome[to_remove:, ...]\n",
        "\n",
        "            experience_buffer_planes[to_move:to_move+num_planes, ...] = np.stack(planes)\n",
        "            experience_buffer_moves[to_move:to_move+num_planes, ...] = np.stack(moves)\n",
        "            experience_buffer_outcome[to_move:to_move+num_planes, ...] = np.repeat(result, num_planes)\n",
        "\n",
        "            tot_num_moves += num_planes\n",
        "            filled_up = max(filled_up+num_planes, conf.MAX_BUFFER_SIZE)\n",
        "\n",
        "    print(\"games\", time()-tic)\n",
        "    return tot_num_moves, filled_up\n",
        "\n",
        "play_parallel_games_apply(\n",
        "    model=fixed_model,\n",
        "    num_games=16,\n",
        "    filled_up=0,\n",
        "    max_depth=4,\n",
        "    num_repetitions=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4OzNXseR-4k"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.TextLineDataset(conf.PATH_ENDGAME_TRAIN_DATASET).shuffle(10000).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# using np.arrays because we add chunks of data and not one at a time (O(n) to move all data is actually O(n/m), with m chunk size)\n",
        "# and also we need to sample randomly batches of data, that for linked lists (like queue) is O(n*batch_zize), instead for arrays is O(batch_size)\n",
        "experience_buffer_planes = np.zeros((conf.MAX_BUFFER_SIZE, *conf.INPUT_SHAPE), dtype=conf.PLANES_DTYPE_NP)  # the bigger the better, check with some experiments\n",
        "experience_buffer_moves = np.zeros((conf.MAX_BUFFER_SIZE, 1), dtype=conf.PLANES_DTYPE_NP)   # the bigger the better, check with some experiments\n",
        "experience_buffer_outcome = np.zeros((conf.MAX_BUFFER_SIZE, 1), dtype=conf.PLANES_DTYPE_NP) # the bigger the better, check with some experiments\n",
        "# start_learning_from = 50000 # number of MOVES\n",
        "\n",
        "# idea: start learning after 50000 samples are in the queue, and randomly select them to pass them through the network, then REMOVE them from the queue\n",
        "# IN PARALLEL, keep playing games with the fixed_model to fill up the queue --> if this step is much faster (or the opposite) --> just wait a bit for the slower one, so that the queue always stays\n",
        "# between ~50k and ~100k\n",
        "\n",
        "# ideally: infinite while loop that launches in parallel two threads/processes:\n",
        "# 1) generates self-play samples (planes, (moves, outcome)) that then get COPIED (otherwise parallelism will screw everything up) and added to the queue\n",
        "# 2) randomly selects batches of 512 samples from the buffer\n",
        "\n",
        "from multiprocessing import Pool\n",
        "from numpy.random import default_rng\n",
        "rng = default_rng()\n",
        "\n",
        "def play_parallel_games_apply(model, num_games, filled_up):\n",
        "    tic = time()\n",
        "    tot_num_moves = 0\n",
        "\n",
        "    starting_positions = dataset.take(num_games)\n",
        "    with Pool() as pool: # play as many games as possible in parallel, and add them to the queue\n",
        "        futures = [pool.apply_async(complete_game, [model, position]) for position in starting_positions]\n",
        "        for f in futures:\n",
        "            print(time()-tic())\n",
        "            planes, moves, result = f.get()\n",
        "            num_planes = len(planes)\n",
        "            \n",
        "            to_remove = min(0, (filled_up + num_planes) - conf.MAX_BUFFER_SIZE)\n",
        "            to_move = conf.MAX_BUFFER_SIZE - to_remove\n",
        "            experience_buffer_planes[:to_move, ...] = experience_buffer_planes[to_remove:, ...]\n",
        "            experience_buffer_moves[:to_move, ...] = experience_buffer_moves[to_remove:, ...]\n",
        "            experience_buffer_outcome[:to_move, ...] = experience_buffer_outcome[to_remove:, ...]\n",
        "\n",
        "            experience_buffer_planes[to_move:to_move+num_planes, ...] = np.stack(planes)\n",
        "            experience_buffer_moves[to_move:to_move+num_planes, ...] = np.stack(moves)\n",
        "            experience_buffer_outcome[to_move:to_move+num_planes, ...] = np.repeat(result, num_planes)\n",
        "\n",
        "            tot_num_moves += num_planes\n",
        "            filled_up = max(filled_up+num_planes, conf.MAX_BUFFER_SIZE)\n",
        "\n",
        "    print(\"games\", time()-tic)\n",
        "    return tot_num_moves, filled_up\n",
        "\n",
        "\n",
        "# def play_parallel_games_imap(model, experience_buffer, num_games):\n",
        "#     tic = time()\n",
        "#     tot_moves = 0\n",
        "#     with Pool() as pool: # play as many games as possible in parallel, and add them to the queue\n",
        "#         futures = pool.imap_unordered(eval, [model]*num_games)\n",
        "#         for f in futures:\n",
        "#             planes, moves, result = f\n",
        "#             tot_moves += (len(planes))\n",
        "#             for plane, move in zip(planes, moves):\n",
        "#                 experience_buffer.append((plane, (move, result)))\n",
        "#     print(\"games\", time()-tic)\n",
        "#     return tot_moves\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def gradient_application(x, y_policy, y_value, model, optimizer, metric):\n",
        "    with tf.GradientTape() as tape:\n",
        "        policy_logits, value_logits = model(x)\n",
        "        policy_loss_value = utils.loss_policy(y_policy, policy_logits)\n",
        "        value_loss_value = utils.loss_value(y_value, value_logits)\n",
        "        loss = policy_loss_value + value_loss_value + sum(model.losses) # to add regularization loss\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    metric.update_state(y_policy, policy_logits)\n",
        "\n",
        "    return policy_loss_value, value_loss_value, loss\n",
        "\n",
        "\n",
        "def train_step(model, optimizer, metric, steps, steps_from_last_stats_print):\n",
        "    tic = time()\n",
        "    total_loss = 0\n",
        "\n",
        "    sample_idxs = rng.choice(range(conf.MAX_BUFFER_SIZE), size=conf.SELF_PLAY_BATCH, replace=False)\n",
        "    # create the batch and remove them from the buffer\n",
        "    planes_batch = [experience_buffer_planes[idx] for idx in sample_idxs] # you don't pop them\n",
        "    moves_batch = [experience_buffer_moves[idx] for idx in sample_idxs] # you don't pop them\n",
        "    outcome_batch = [experience_buffer_outcome[idx] for idx in sample_idxs] # you don't pop them\n",
        "        \n",
        "    policy_loss_value, value_loss_value, loss = gradient_application(planes_batch, moves_batch, outcome_batch, model, optimizer, metric)\n",
        "\n",
        "    if steps_from_last_stats_print > conf.STEPS_PER_UPDATE:\n",
        "        steps_from_last_stats_print = 0\n",
        "\n",
        "        to_print = [\"step: \", steps]\n",
        "        to_print.append(\"- policy loss (instantaneous): \")\n",
        "        to_print.append(policy_loss_value)\n",
        "        to_print.append(\"- value_loss (instantaneous): \")\n",
        "        to_print.append(value_loss_value)\n",
        "        to_print.append(\"- loss (instantaneous): \")\n",
        "        to_print.append(loss)\n",
        "        to_print.append(\"- policy accuracy: \")\n",
        "        to_print.append(metric.result())\n",
        "        print(to_print)\n",
        "\n",
        "        metric.reset_states()\n",
        "\n",
        "    total_loss += loss\n",
        "\n",
        "    print(time()-tic)\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "def train_loop():\n",
        "    steps = 0\n",
        "    filled_up = 0\n",
        "    steps_from_last_model_update = 0\n",
        "    steps_from_last_eval = 0\n",
        "    steps_from_last_stats_print = 0\n",
        "\n",
        "    fixed_model = tf.keras.models.load_model(conf.PATH_FIXED_MODEL)\n",
        "    updating_model = tf.keras.models.load_model(conf.PATH_UPDATING_MODEL)\n",
        "\n",
        "    tb_callback = tf.keras.callbacks.TensorBoard(\n",
        "        log_dir = \"logs/self-play\", \n",
        "        write_graph = False,\n",
        "        write_steps_per_second = True,\n",
        "        update_freq = conf.STEPS_PER_UPDATE\n",
        "    )\n",
        "    tb_callback.set_model(updating_model)\n",
        "\n",
        "    ##### ALREADY COMPILED MODEL, DO WE NEED THIS?\n",
        "    # lr_boundaries = [100000, 300000, 500000]    # from paper\n",
        "    # lr_values = [0.2, 0.02, 0.002, 0.0002]      # from paper\n",
        "    # lr_scheduler = tf.keras.optimizers.schedules.PiecewiseConstantDecay(lr_boundaries, lr_values)\n",
        "    # optimizer = tf.keras.optimizers.Adam(learning_rate = lr_scheduler)\n",
        "\n",
        "    # loss_policy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # from paper\n",
        "    # loss_value = tf.keras.losses.MeanSquaredError()                     # from paper\n",
        "\n",
        "    # metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "    tot_moves = 0\n",
        "    tot_games = 0\n",
        "\n",
        "    while steps < conf.TOTAL_STEPS:\n",
        "\n",
        "        n_moves, filled_up = play_parallel_games_apply(fixed_model, conf.NUM_PARALLEL_GAMES, filled_up)\n",
        "        \n",
        "        tot_moves += n_moves\n",
        "        tot_games += conf.NUM_PARALLEL_GAMES\n",
        "        mean_length_game = tot_moves/tot_games\n",
        "\n",
        "        for i in conf.NUM_TRAINING_STEPS:\n",
        "            cycle_loss, cycle_steps = train_step(updating_model, filled_up, optimizer, loss_policy, loss_value, metric, steps, steps_from_last_stats_print)\n",
        "\n",
        "        if steps_from_last_model_update > conf.STEPS_PER_UPDATE:\n",
        "            steps_from_last_model_update = 0\n",
        "\n",
        "            updating_model.save(conf.PATH_UPDATING_MODEL, save_traces=False) # should decrease saving time, since we don't have custome layers/models\n",
        "            fixed_model = tf.keras.models.load_model(conf.PATH_UPDATING_MODEL)\n",
        "        \n",
        "        if steps_from_last_eval > conf.STEPS_PER_EVAL_CKPT == 0:\n",
        "            steps_from_last_eval = 0\n",
        "\n",
        "            updating_model.save(conf.PATH_CKPT_FOR_EVAL.format(steps), save_traces=False)\n",
        "\n",
        "        steps += conf.NUM_TRAINING_STEPS\n",
        "        steps_from_last_model_update += conf.NUM_TRAINING_STEPS\n",
        "        steps_from_last_eval += conf.NUM_TRAINING_STEPS\n",
        "        steps_from_last_stats_print += conf.NUM_TRAINING_STEPS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfFkDLWvvjLb"
      },
      "outputs": [],
      "source": [
        "# TODO: try num games in parallel vs. time\n",
        "# TODO: see if you can evaluate until games finish playing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et8v7NWjpPlO",
        "outputId": "92b54c44-0eea-4fd8-a7f0-7b88cb25ee7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 loop, best of 5: 47.4 s per loop\n",
            "1 loop, best of 5: 47.1 s per loop\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from time import time, sleep\n",
        "from multiprocessing import Pool\n",
        "\n",
        "input = tf.keras.layers.Input(shape=(1))\n",
        "output = tf.keras.layers.Dense(1)(input)\n",
        "\n",
        "model = tf.keras.Model(inputs=input, outputs=output)\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer = 'adam'\n",
        ")\n",
        "\n",
        "def eval(model):\n",
        "    # print(time())\n",
        "    sleep(np.random.random()*5)\n",
        "    return [model(np.zeros(1)), model(np.ones(1))]\n",
        "\n",
        "def play_parallel_games_apply(model, experience_buffer, num_games):\n",
        "    with Pool() as pool: # play as many games as possible in parallel, and add them to the queue\n",
        "        futures = [pool.apply_async(eval, [model]) for w in range(num_games)]\n",
        "        for f in futures:\n",
        "            result = f.get()\n",
        "            for part in result:\n",
        "                experience_buffer.append(part)\n",
        "\n",
        "def play_parallel_games_imap(model, experience_buffer, num_games):\n",
        "    with Pool() as pool: # play as many games as possible in parallel, and add them to the queue\n",
        "        futures = pool.imap_unordered(eval, [model]*num_games)\n",
        "        for f in futures:\n",
        "            result = f\n",
        "            for part in result:\n",
        "                experience_buffer.append(part)\n",
        "\n",
        "l = []\n",
        "%timeit play_parallel_games_apply(model, l, 50)\n",
        "l = []\n",
        "%timeit play_parallel_games_imap(model, l, 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMM-P-_j18DM"
      },
      "outputs": [],
      "source": [
        "utils.gen()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "unTlsTuBEaBU",
        "outputId": "13f7996e-1cbe-4d8d-fcae-0b8c45279149"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4911.572229639013\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-bbd0d0f40c80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mbce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m73\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'log' is not defined"
          ]
        }
      ],
      "source": [
        "from numpy.random import default_rng\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "qty = 10000\n",
        "\n",
        "rng = default_rng()\n",
        "labels = np.zeros((8*8*73))\n",
        "labels[0] = 1\n",
        "labels = [labels]*qty\n",
        "predict_array = np.zeros((8*8*73))\n",
        "predictions = []\n",
        "for i in range(qty):\n",
        "    predict_array = np.random.random_sample((8*8*73))*2-1\n",
        "    # predict_array /= np.sum(predict_array)\n",
        "    predictions.append(predict_array)\n",
        "    # idx = rng.choice(range(8*8*73))\n",
        "    # predict_array[idx] = 1\n",
        "    # predictions.append(predict_array.copy())\n",
        "    # predict_array[idx] = 0\n",
        "\n",
        "print(np.sum([np.sum(p) for p in predictions]))\n",
        "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "bce(labels, predictions).numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFUrtHf_Pg9B",
        "outputId": "4600f6b1-9c82-43e1-d4a9-140661159ed6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]], shape=(8, 4672), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "qty = 10000\n",
        "\n",
        "# labels = np.zeros((qty,3))\n",
        "# labels[:, 0] = 45\n",
        "# labels[:, 1] = 1\n",
        "# labels[:, 2] = 300\n",
        "\n",
        "# predictions = np.random.rand(qty, 8, 8, 73)\n",
        "# print(labels.shape)\n",
        "# print(predictions.shape)\n",
        "# # bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "# # bce(labels, predictions).numpy()\n",
        "\n",
        "# cce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "# cce(labels, predictions).numpy()\n",
        "\n",
        "x = np.zeros((8, 8, 8, 73))\n",
        "x[0, 2, 3, 5] = 1\n",
        "# x= np.array(x)\n",
        "\n",
        "\n",
        "lay = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "print(lay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hT4ZN_PQOro",
        "outputId": "0291c026-7c08-4b12-ede0-67468fd7d5cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0\n",
            "index 1392\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "arr_x = 2\n",
        "arr_y = 3\n",
        "arr_z = 5\n",
        "\n",
        "print(x[0, arr_x, arr_y, arr_z])\n",
        "idx = np.ravel_multi_index([arr_x, arr_y, arr_z], np.shape(x)[1:])\n",
        "print(\"index\", idx)\n",
        "print(lay[0][idx].numpy() == x[0, arr_x, arr_y, arr_z])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 ('env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "6ff546d1c5a7064c8e32c19edaef78491647a0db70d1b75d69edec23d383707d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
